# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('ggplot')
%matplotlib inline

# %% [markdown]
# ## 1. Data Loading and Initial Exploration

# %%
# Load the dataset
try:
    nutrition_df = pd.read_csv('nutrition_dataset.csv')
    print("Dataset loaded successfully with shape:", nutrition_df.shape)
except FileNotFoundError:
    print("Error: File 'nutrition_dataset.csv' not found. Please ensure the file is in the correct directory.")
    raise

# Display first few rows
display(nutrition_df.head())

# Basic information about the dataset
print("\nDataset Information:")
nutrition_df.info()

# Statistical summary
print("\nStatistical Summary:")
display(nutrition_df.describe(include='all'))

# Check for missing values
print("\nMissing Values:")
display(nutrition_df.isnull().sum())

# %% [markdown]
# ## 2. Data Cleaning and Preprocessing

# %%
# Handle missing values
def clean_data(df):
    # Drop columns with more than 50% missing values
    threshold = len(df) * 0.5
    df_cleaned = df.dropna(thresh=threshold, axis=1)
    
    # For numerical columns, fill missing values with median
    num_cols = df_cleaned.select_dtypes(include=['int64', 'float64']).columns
    for col in num_cols:
        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)
    
    # For categorical columns, fill missing values with mode
    cat_cols = df_cleaned.select_dtypes(include=['object']).columns
    for col in cat_cols:
        df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)
    
    return df_cleaned

nutrition_cleaned = clean_data(nutrition_df)
print("Data cleaned. Remaining missing values:", nutrition_cleaned.isnull().sum().sum())

# %% [markdown]
# ## 3. Exploratory Data Analysis (EDA)

# %%
# Visualize distribution of numerical features
def plot_distributions(df, cols=5):
    num_cols = df.select_dtypes(include=['int64', 'float64']).columns
    rows = len(num_cols) // cols + 1
    
    plt.figure(figsize=(20, 5*rows))
    for i, col in enumerate(num_cols):
        plt.subplot(rows, cols, i+1)
        sns.histplot(df[col], kde=True)
        plt.title(f'Distribution of {col}')
        plt.tight_layout()
    
    plt.show()

plot_distributions(nutrition_cleaned)

# %%
# Correlation analysis
plt.figure(figsize=(15, 10))
corr_matrix = nutrition_cleaned.select_dtypes(include=['int64', 'float64']).corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# %%
# Pairplot for top correlated features
top_features = corr_matrix.abs().unstack().sort_values(ascending=False).reset_index()
top_features = top_features[top_features['level_0'] != top_features['level_1']]
top_5_pairs = top_features.head(10)['level_0'].unique()[:5]

sns.pairplot(nutrition_cleaned[top_5_pairs])
plt.suptitle('Pairplot of Top Correlated Features', y=1.02)
plt.show()

# %% [markdown]
# ## 4. Feature Engineering

# %%
# Create new features that might be nutritionally relevant
def create_features(df):
    # Calculate energy balance (calories - (protein*4 + carbs*4 + fat*9))
    if all(col in df.columns for col in ['Calories', 'Protein', 'Carbohydrates', 'Fat']):
        df['Energy_Balance'] = df['Calories'] - (df['Protein']*4 + df['Carbohydrates']*4 + df['Fat']*9)
    
    # Create nutrient density score (micronutrients per calorie)
    micronutrients = ['Vitamin_A', 'Vitamin_C', 'Calcium', 'Iron']  # Adjust based on your dataset
    micronutrients = [col for col in micronutrients if col in df.columns]
    
    if micronutrients and 'Calories' in df.columns:
        df['Nutrient_Density'] = df[micronutrients].sum(axis=1) / (df['Calories'] + 1)  # +1 to avoid division by zero
    
    # Create macronutrient ratios
    if all(col in df.columns for col in ['Protein', 'Carbohydrates', 'Fat']):
        df['Protein_Ratio'] = df['Protein'] / (df['Protein'] + df['Carbohydrates'] + df['Fat'])
        df['Carb_Ratio'] = df['Carbohydrates'] / (df['Protein'] + df['Carbohydrates'] + df['Fat'])
        df['Fat_Ratio'] = df['Fat'] / (df['Protein'] + df['Carbohydrates'] + df['Fat'])
    
    return df

nutrition_features = create_features(nutrition_cleaned)
display(nutrition_features.head())

# %% [markdown]
# ## 5. Data Preparation for Modeling

# %%
# Select target and features
# Assuming we want to predict 'Calories' based on other nutritional components
# Adjust based on your specific dataset and goals
target = 'Calories'
if target not in nutrition_features.columns:
    target = nutrition_features.columns[-1]  # Fallback to last column if target not found

features = [col for col in nutrition_features.columns if col != target and nutrition_features[col].dtype in ['int64', 'float64']]

X = nutrition_features[features]
y = nutrition_features[target]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}")

# %% [markdown]
# ## 6. Feature Selection

# %%
# Select top 10 features using SelectKBest
selector = SelectKBest(score_func=f_regression, k=10)
selector.fit(X_train_scaled, y_train)

selected_features = X_train.columns[selector.get_support()]
print("Selected Features:", selected_features.tolist())

# Update X_train and X_test with selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Scale the selected features
X_train_selected_scaled = scaler.fit_transform(X_train_selected)
X_test_selected_scaled = scaler.transform(X_test_selected)

# %% [markdown]
# ## 7. Dimensionality Reduction with PCA

# %%
# Apply PCA to visualize feature reduction
pca = PCA()
pca.fit(X_train_scaled)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_)+1), 
         np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.grid(True)
plt.show()

# Choose number of components explaining 95% variance
n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1
print(f"Number of components explaining 95% variance: {n_components}")

# Apply PCA with selected components
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# %% [markdown]
# ## 8. Model Building and Evaluation

# %%
# Define models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42)
}

# Evaluate each model
results = {}
for name, model in models.items():
    # Train model
    model.fit(X_train_selected_scaled, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test_selected_scaled)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Store results
    results[name] = {
        'Model': model,
        'MSE': mse,
        'RMSE': rmse,
        'R2': r2
    }
    
    # Print results
    print(f"{name} Results:")
    print(f"  MSE: {mse:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R2 Score: {r2:.2f}")
    print()

# %% [markdown]
# ## 9. Hyperparameter Tuning for Best Model

# %%
# Select best model based on R2 score
best_model_name = max(results.items(), key=lambda x: x[1]['R2'])[0]
print(f"Best model: {best_model_name}")

# Set up parameter grid for Random Forest
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    # Grid search with cross-validation
    grid_search = GridSearchCV(
        estimator=RandomForestRegressor(random_state=42),
        param_grid=param_grid,
        cv=5,
        scoring='r2',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train_selected_scaled, y_train)
    
    # Get best model
    best_model = grid_search.best_estimator_
    results['Tuned Random Forest'] = {
        'Model': best_model,
        'MSE': mean_squared_error(y_test, best_model.predict(X_test_selected_scaled)),
        'RMSE': np.sqrt(mean_squared_error(y_test, best_model.predict(X_test_selected_scaled))),
        'R2': r2_score(y_test, best_model.predict(X_test_selected_scaled))
    }
    
    print("\nBest Parameters:", grid_search.best_params_)
    print("Tuned Random Forest R2 Score:", results['Tuned Random Forest']['R2'])

# %% [markdown]
# ## 10. Feature Importance Analysis

# %%
# Plot feature importances for tree-based models
if best_model_name in ['Random Forest', 'Tuned Random Forest']:
    model = results[best_model_name]['Model']
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(12, 6))
    plt.title("Feature Importances")
    plt.bar(range(X_train_selected.shape[1]), importances[indices], align="center")
    plt.xticks(range(X_train_selected.shape[1]), selected_features[indices], rotation=90)
    plt.xlim([-1, X_train_selected.shape[1]])
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 11. Final Model Evaluation and Interpretation

# %%
# Select the final best model
final_model = results[max(results.items(), key=lambda x: x[1]['R2'])[0]]['Model']

# Make final predictions
y_pred = final_model.predict(X_test_selected_scaled)

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values')
plt.show()

# Residual plot
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='-')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

# %% [markdown]
# ## 12. Saving the Model and Results

# %%
import joblib
import datetime

# Save the final model
model_filename = f"nutrition_model_{datetime.datetime.now().strftime('%Y%m%d')}.pkl"
joblib.dump(final_model, model_filename)
print(f"Model saved as {model_filename}")

# Save the scaler
scaler_filename = f"nutrition_scaler_{datetime.datetime.now().strftime('%Y%m%d')}.pkl"
joblib.dump(scaler, scaler_filename)
print(f"Scaler saved as {scaler_filename}")

# Save feature names
features_filename = f"nutrition_features_{datetime.datetime.now().strftime('%Y%m%d')}.pkl"
joblib.dump(selected_features.tolist(), features_filename)
print(f"Feature names saved as {features_filename}")

# Save results to CSV
results_df = pd.DataFrame.from_dict(results, orient='index')
results_df.drop(columns=['Model'], inplace=True)  # Don't save the model objects
results_filename = f"nutrition_results_{datetime.datetime.now().strftime('%Y%m%d')}.csv"
results_df.to_csv(results_filename)
print(f"Results saved as {results_filename}")

# %% [markdown]
# ## 13. Conclusion and Next Steps
# 
# This analysis provides a comprehensive approach to predicting nutritional values based on a dataset. The key findings are:
# 
# 1. The best performing model was {best_model_name} with an R2 score of {results[best_model_name]['R2']:.2f}.
# 2. The most important features were {selected_features[:3]} (and others shown in the feature importance plot).
# 3. The model can be improved further by:
#    - Collecting more data
#    - Engineering additional relevant features
#    - Trying more complex models like Gradient Boosting or Neural Networks
#    - Incorporating domain knowledge for better feature selection
# 
# The saved model can now be deployed to predict nutritional values for new food items.

# %%
# Display final results
print("\nFinal Model Performance:")
display(results_df)
